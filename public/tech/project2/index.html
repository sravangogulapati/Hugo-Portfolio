<!DOCTYPE html>
<html><head>
    <meta charset="UTF-8" />
    <link rel="stylesheet" type="text/css" href="/css/main_styles.css" />
    
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Sacramento&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Pangolin&display=swap" rel="stylesheet">
</head><body>
        <div id="content">

    
    
<div class="header">
    <div class="logo-container">
        <a href="https://imsravan.com/" class="logo">Sai Sravan Gogulapati</a>
    </div>
    
    <div class="header-right">
        
            <a href="https://imsravan.com/">Home</a>
        
        
            
                
                    <a href="https://imsravan.com/art/">Art</a>
                
            
        
            
        
            
        
            
        
            
        
            
                
                    <a href="https://imsravan.com/tech/">Tech</a>
                
            
        
            
        
            
        
            
        
            
        
    </div>
</div>
<div class="header-spacer"></div>

    <a href='https://imsravan.com/tech'>&lt; All tech</a>
    <h1>iSMuG - Senior Design</h1>
    <p>
        <p><em>Interactive Smart Music Generator (iSMuG) is a novel music generator based on human states observed via computer vision (CV) and biomarker sensors.</em>
Senior Design Project by Sai Sravan Gogulapati, Kenny Moc, Arushi Arora, and Jade Whiting.</p>
<p>Our interactive smart music generator employs novel AI music generation and interactive feedback systems based on a user’s human state, which is detected via computer vision and physiological sensors like a heart rate monitor or galvanic skin response sensor. One of the main reasons we came up with this idea was because of our shared interests in both machine learning and its applications and music in general. We also thought this project would serve as a great proof of concept for companies like startups for autonomous cars, which are currently working on interactive music recommendation systems based on the passengers’ emotional states as monitored through computer vision. Our project can additionally be used as an open-ended tool for other research pertaining to the psychology of music and its effect on people’s emotions, which is an area that’s widely researched and of a lot of interest in the research world.</p>
<p>Both before and during our project work, we read a lot of the literature pertaining to the intersection of music or art and AI. Emotions are often vague, complex, and ambiguous so it’s hard to adjust generated art and music to match a subjective taste and incorporate preference learning with that. Since our project revolves around identifying features in the music that we can tweak to ensure it reflects the user’s mood, we read about features like dynamics, pitch, rhythm, and things like that to see how that influences people’s emotions. In a lot of these prior works, the ground-truth mood of a piece of music was typically extracted via low-level audio features like the ones mentioned, along with metadata about the piece.</p>

    </p>


        </div></body>
</html>
